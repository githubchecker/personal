# Docker

## **Phase 1: The Philosophy & Vocabulary**

Welcome to the start of your Docker journey.

As a .NET developer, you likely think of "deploying" as copying files to a server or clicking "Publish" in Visual Studio to IIS. Docker changes this mental model completely. We stop shipping **Artifacts** (.zip files) and start shipping **Environments**.

Here is the deep dive into Phase 1 concepts.

---

### **1. Image vs. Container (The Class vs. Object)**

This is the most fundamental distinction. If you confuse these two, nothing else will make sense.

### **The Analogy for C# Developers**

Think of an **Image** like a **C# Class definition**.
Think of a **Container** like an **Instance (Object)** of that class.

- **The Image (`public class WebApp`)**:
    - It is the **Blueprint**.
    - It is **Immutable** (Read-Only). You cannot change an image once it is built; you have to build a new one (just like you recompile code).
    - It contains *everything* the app needs to run: The specific version of Windows/Linux, the .NET Runtime, dependent libraries, and your Compiled DLLs.
- **The Container (`var app = new WebApp()`):**
    - It is the **Runtime Instance**.
    - You can create 100 containers from 1 Image.
    - It is **Ephemeral** (Temporary). If you delete a container, itâ€™s gone. If you spin up a new one from the same Image, it starts fresh.

### **Why does this matter?**

In the old world (VMs/IIS), if you logged into the production server and manually tweaked a `web.config` file, that server became unique ("Snowflake server"). If the server crashed, you lost those tweaks.

In Docker, because the Image is immutable, **it is impossible to "drift"**. The exact same Image hash that ran on your laptop runs in Azure.

---

### **2. The Architecture (Daemon vs. Client)**

When you install "Docker Desktop" on Windows, you are actually installing two things:

1. **The Docker Daemon (The Server/Engine):**
    - This is a background service. It does the heavy lifting (building images, running containers).
    - On Windows, this usually runs inside a hidden lightweight Linux VM (WSL2).
2. **The Docker CLI (The Client):**
    - This is the command `docker` you type in PowerShell.
    - **Crucial Concept:** The CLI does nothing itself. It sends **REST API** instructions to the Daemon.

### **The Registry (The "NuGet" for Images)**

Where does the Daemon get the base software?

- **NuGet** stores library packages (`.nupkg`).
- **Docker Registry** stores image packages.

**Common Registries:**

- **Docker Hub:** The default (Public). Hosting Nginx, Redis, and official .NET images.
- **MCR (Microsoft Container Registry):** `mcr.microsoft.com`. Hosting official .NET images.
- **ACR (Azure Container Registry):** Your private, secure registry in Azure (we will cover this later).

---

### **3. Port Mapping (The Locked Room)**

By default, a Container is a paranoid, isolated jail. It allows outgoing traffic (so your app can call Azure SQL), but it blocks **all** incoming traffic.

If your [ASP.NET](http://asp.net/) Core app starts up inside a container listening on port `80`, **you cannot access it** from your browser on `localhost:80`. The container's port 80 is isolated.

You must "punch a hole" in the wall using **Port Mapping (`-p`)**.

### **The Syntax: `p Host:Container`**

- **Host Port:** The port on your physical laptop (External).
- **Container Port:** The port the app is listening on inside the container (Internal).

**Example:**`docker run -p 8080:80 my-site`

- Request -> Laptop Port `8080` -> Magic Tunnel -> Container Port `80` -> Kestrel Server.
- Browser URL: `http://localhost:8080`

---

### **Practical Exercise: Your First "Run"**

Open PowerShell or Command Prompt. Let's run a sample [ASP.NET](http://asp.net/) Core app provided by Microsoft.

**Command:**

```powershell
# Syntax: docker run [Options] [ImageName]
# -d: Detached mode (Run in background, don't lock my terminal)
# -p: Port Map (My machine 8000 -> Container 8080)
# --name: Give it a friendly name so we can stop it later

docker run -d -p 8000:8080 --name my-first-app mcr.microsoft.com/dotnet/samples:aspnetapp

```

**What just happened?**

1. **Client:** Sent request to Daemon.
2. **Daemon:** Checked local cache for image `mcr.microsoft.com/...`. Not found.
3. **Daemon:** Downloaded (Pulled) the layers from the Microsoft Registry.
4. **Daemon:** Created a container instance.
5. **Daemon:** Mapped your PC's port 8000 to the container's standard 8080.

**Test It:**
Open your browser to: `http://localhost:8000`. You should see the .NET Welcome Page.

**Cleanup:**
Now, destroy the object (Container).

```powershell
docker stop my-first-app
docker rm my-first-app

```

---

### **Summary of Phase 1**

1. **Immutability:** Images never change. If you need to change code, you build a NEW image.
2. **Isolation:** Containers don't know they are running on your laptop. They think they are the only thing running on a clean Linux OS.
3. **Networking:** Nothing gets in unless you explicitly open a port via `p`.

## **Phase 2: Writing the Blueprint (The Dockerfile)**

In Phase 1, we ran an image created by Microsoft. Now, we create our own image for **your** [ASP.NET](http://asp.net/) Core Web API.

The **Dockerfile** (no extension) is a text document that contains all the commands a user could call on the command line to assemble an image. Think of it as a script: "Start with Windows, install .NET, copy my files, and start the app."

---

### **1. The Base Image (`FROM`)**

Every image must inherit from a parent. For .NET, Microsoft provides two main flavors on the **Microsoft Container Registry (MCR)**.

- **The SDK Image** (`mcr.microsoft.com/dotnet/sdk:8.0`)
    - **Contains:** .NET Runtime + Compilers (Roslyn), CLI tools, NuGet.
    - **Size:** Heavy (~800MB).
    - **Use Case:** Building code (compiling C# to DLLs).
- **The Runtime Image** (`mcr.microsoft.com/dotnet/aspnet:8.0`)
    - **Contains:** .NET Runtime only. No compilers.
    - **Size:** Light (~100MB).
    - **Use Case:** Running the compiled application in production.

**Syntax:**

```docker
FROM mcr.microsoft.com/dotnet/sdk:8.0

```

---

### **2. Setting the Stage (`WORKDIR`)**

Linux file systems start at root `/`. You don't want to dump your application files in the root folder alongside system files.

**The Command:** `WORKDIR /app`

- This acts like `mkdir /app` AND `cd /app`.
- All future commands (COPY, RUN) happen inside this folder.

---

### **3. Moving Files (`COPY`)**

You need to get your source code from your laptop (Host) into the image (Container).

**Syntax:** `COPY [Source_On_Laptop] [Destination_In_Image]`

```docker
# Copy everything from the current folder on laptop (.) to the current workdir in container (.)
COPY . .

```

### **The "Build Context" Trap (Crucial)**

Novices often try: `COPY ../MySharedLibrary /app`**This fails.** Docker can only see files *inside* the folder where you run the `docker build` command (and its subfolders). It cannot reach "up" a directory level for security reasons.

---

### **4. Executing Commands (`RUN`)**

This command executes a terminal instruction **during the build process**. It commits the result to the image.

For .NET, we need to restore packages and build the code.

```docker
# Download NuGet packages
RUN dotnet restore

# Compile and Create the DLLs (Output to /app/publish folder)
RUN dotnet publish -c Release -o /app/publish

```

---

### **5. Starting the App (`ENTRYPOINT`)**

This is the command that runs when the **Container starts** (Runtime).

**Syntax:** `ENTRYPOINT ["executable", "argument"]`

```docker
# Change directory to where we published the files
WORKDIR /app/publish

# Define the start command
ENTRYPOINT ["dotnet", "MyApi.dll"]

```

- **Note:** Use the array format `["..."]`. It handles system signals (like "Stop") correctly.

---

### **Practical Exercise: The "Fat" Image**

Let's put it together into a working Dockerfile. We will create a "Single-Stage" build.

*Note: This creates a large image because we are including the SDK compilers in the final product. We will fix this in Phase 3.*

**The File:** `Dockerfile`

```docker
# 1. Base Image (Heavy SDK)
FROM mcr.microsoft.com/dotnet/sdk:8.0

# 2. Setup Work Directory
WORKDIR /src

# 3. Copy Code
COPY . .

# 4. Restore & Publish
RUN dotnet restore
RUN dotnet publish -c Release -o /app/publish

# 5. Prepare for Runtime
WORKDIR /app/publish
EXPOSE 8080

# 6. Start
ENTRYPOINT ["dotnet", "MyApi.dll"]

```

**How to Build and Run it:**

1. **Build:**
Run this in the folder containing the Dockerfile.
    
    ```powershell
    # -t: Tag (Name) the image
    # . : The "Context" (Where to look for files)
    docker build -t my-fat-api .
    
    ```
    
2. **Run:**
    
    ```powershell
    docker run -p 5000:8080 my-fat-api
    
    ```
    

---

### **Summary of Phase 2**

1. **`FROM`**: Pick your OS/Runtime.
2. **`WORKDIR`**: Always set this first to keep things organized.
3. **`COPY`**: Bring files in. Remember the "Context" limit.
4. **`RUN`**: Execute build commands (compile).
5. **`ENTRYPOINT`**: Define what happens when the container wakes up.

**The Problem:**
If you run `docker images`, you will see `my-fat-api` is likely **800MB+**. This is too big for production. It takes too long to upload to Azure, and it contains SDK tools that hackers could use.

Are you ready for **Phase 3: The Holy Grail - Multi-Stage Builds** (Where we shrink this to 80MB)?

## **Phase 3: The Holy Grail - Multi-Stage Builds (Intermediate)**

This is the most critical concept for a .NET developer.

**The Problem with Phase 2:**
In the previous phase, we used `FROM mcr.microsoft.com/dotnet/sdk`.

- **Result:** You produced an image that works, but it weighs **~800MB**.
- **Why?** You included the C# Compiler (`Roslyn`), `MSBuild`, and the entire CLI tooling inside your production container.
- **Risks:**
    1. **Bloat:** Slow to push to Azure, slow to pull on new nodes.
    2. **Security:** If a hacker gets into your container, they have a compiler! They can compile malicious code right inside your production server.

**The Solution:**
We separate the **Building** (which needs the SDK) from the **Running** (which only needs the Runtime).

---

### **1. The Concept: Two Stages, One File**

Docker allows you to use multiple `FROM` statements in a single Dockerfile.

1. **Stage 1 (Alias `build`):** Use the heavy SDK image. Compile the code. Produce DLLs.
2. **Stage 2 (Alias `final`):** Start fresh with the light [ASP.NET](http://asp.net/) Runtime image.
3. **The Bridge:** Copy **only** the compiled DLLs from Stage 1 to Stage 2. Discard Stage 1 entirely.

---

### **2. Layer Caching (The "Speed" Optimization)**

Before we write the final file, you must understand **Caching**.

Docker executes lines top-to-bottom. If a file used in a line hasn't changed, Docker **skips that step** and uses a cached result.

**The Novice Way:**

```docker
COPY . .            # <--- If you change ONE .cs file (Logic)...
RUN dotnet restore  # <--- Docker must re-download ALL NuGet packages! (Slow)

```

**The Expert Way:**
We copy the Project file (`.csproj`) first, because dependencies change rarely. We copy the Code files (`.cs`) later.

```docker
COPY MyApi.csproj .
RUN dotnet restore  # <--- Docker caches this layer forever unless you add a NuGet package.
COPY . .            # <--- Now copy code.
RUN dotnet build    # <--- Only this runs when code changes. Fast!

```

---

### **3. The Expert Dockerfile (.NET 8)**

Here is the production-standard pattern you should memorize.

```docker
# -------------------------------------------------------------------
# STAGE 1: THE BUILDER
# -------------------------------------------------------------------
FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build
WORKDIR /src

# OPTIMIZATION: Copy csproj and restore as distinct layers
COPY ["MyApi.csproj", "./"]
RUN dotnet restore "MyApi.csproj"

# Copy the rest of the source code
COPY . .

# Build and Publish (Release mode)
# -o /app/publish: Output DLLs to a specific folder
RUN dotnet publish "MyApi.csproj" -c Release -o /app/publish /p:UseAppHost=false

# -------------------------------------------------------------------
# STAGE 2: THE RUNNER (Production Image)
# -------------------------------------------------------------------
FROM mcr.microsoft.com/dotnet/aspnet:8.0 AS final
WORKDIR /app

# SECURITY: Run as a non-root user (Best Practice)
# (Pre-installed 'app' user available in .NET 8 images)
USER app

# NETWORKING: .NET 8 defaults to port 8080 (not 80)
EXPOSE 8080

# THE BRIDGE: Copy compiled artifacts from the 'build' stage
# This implies: "Go to the machine named 'build', grab the folder /app/publish, put it here."
COPY --from=build /app/publish .

# Start the app
ENTRYPOINT ["dotnet", "MyApi.dll"]

```

---

### **4. Analysis of the Result**

1. **Stage 1 (Build):** Downloads the heavy SDK (800MB), downloads NuGet packages, compiles code. **All of this is discarded.**
2. **Stage 2 (Final):** Downloads the light Runtime (100MB). Receives *only* the ~20MB of DLLs from Stage 1.
3. **Total Size:** ~120MB (vs 800MB).
4. **Security:** No compilers, no source code, and running as a non-root user (`USER app`).

### **5. Build Context: Where do I run this?**

A common mistake is directory structure.
Standard .NET structure:

```
/RepoRoot
  |-- MySolution.sln
  |-- /MyApi
       |-- MyApi.csproj
       |-- Program.cs
       |-- Dockerfile  <-- INSIDE PROJECT FOLDER?

```

**If the Dockerfile is inside the project folder:**
You run: `docker build -t my-optimized-api .` inside `/MyApi`.

**If you have multiple projects (Class Libraries):**
You usually move the Dockerfile to the **Repo Root** so it can "see" all referenced library folders during the `COPY` step.

- *Correction:* In enterprise apps, the Dockerfile usually stays in the root, and the `COPY` paths look like `COPY ["src/MyApi/MyApi.csproj", "src/MyApi/"]`.

---

### **Summary of Phase 3**

1. **Multiple FROMs:** Use `AS alias` to name stages.
2. **Specific COPY:** Use `COPY --from=alias` to extract artifacts.
3. **Optimization:** Restore *before* copying full source code to leverage caching.
4. **User App:** Always switch to `USER app` (or similar) to prevent Root exploits.

Are you ready to move to **Phase 4: Persistence & Networking**? (Where we handle the tricky issue of Databases and "Localhost" confusion).

## **Phase 4: Persistence & Networking**

In Phase 3, we built a perfect, immutable image. Now we run it.

Here is the reality check: **Containers have amnesia.**
If you restart a container, the file system resets to exactly how it was in the Image. If you wrote a log file or saved a user to a database *inside* the container, that data is gone forever.

Also, **Containers are lonely.**
By default, they can't talk to each other easily. Your Web API container doesn't know your SQL container exists.

Phase 4 fixes these two problems.

---

### **1. Persistence: Saving Data (Volumes)**

**The Scenario:**
You are running SQL Server in a Docker container for development. You create a database, add a table, and insert 5 rows. You stop the container for the night.
Next morning, you start it up. **The database is gone.**

**The Reason:**
The database files were written inside the container's writable layer. When the container stopped/removed, that layer vanished.

**The Solution: Volumes**
We must punch a hole in the container to map a folder *inside* the container to a folder on your *physical hard drive*.

### **Type A: Named Volumes (Best for Databases)**

Docker manages a storage area on your disk (usually hidden in `var/lib/docker/volumes`). You don't care *exactly* where it is, you just give it a name.

**The Syntax:** `-v VolumeName:PathInsideContainer`

**Example (Running SQL Server):**
The official SQL image stores data at `/var/opt/mssql/data`. We map that.

```powershell
docker run -d `
  --name my-sql `
  -e "ACCEPT_EULA=Y" `
  -e "SA_PASSWORD=YourStrong!Passw0rd" `
  -p 1433:1433 `
  -v sql_data_vol:/var/opt/mssql/data ` # <--- THE MAGIC LINE
  mcr.microsoft.com/mssql/server:2022-latest

```

- **Result:** Even if you delete the container `my-sql`, the volume `sql_data_vol` survives. If you spin up a *new* container and attach the same volume, your data comes back!

### **Type B: Bind Mounts (Best for Config/Code)**

You map a specific path on your C: drive to the container. Use this to inject `appsettings.json` or certificates without rebuilding the image.

**Example:**

```powershell
# Map C:\\Projects\\Configs\\appsettings.json TO /app/appsettings.json
docker run -v C:\\Projects\\Configs\\appsettings.json:/app/appsettings.json my-api

```

---

### **2. Networking: The "Localhost" Trap**

**The Scenario:**

- You have **SQL Server** running on your laptop (Host) on port 1433.
- You run your **Web API** in a container.
- Connection String: `Server=localhost;...`

**The Error:** `SqlException: A network-related or instance-specific error occurred...`

**The Reason:**
Inside the container, `localhost` means **The Container Itself**. It is looking for SQL Server *inside* the API container. It is not looking at your laptop.

### **Solution A: Calling the Host (Quick Fix)**

If you want the container to talk to your physical laptop (where the old SQL instance is running), do not use `localhost`.
Use the special DNS name: **`host.docker.internal`**

- **Connection String:** `Server=host.docker.internal;Database=...`

### **Solution B: Bridge Networks (The Proper Way)**

If *both* the API and the SQL are running in containers, `host.docker.internal` is sloppy. We want them to talk directly to each other securely.

1. **Create a Network:**
Think of this like plugging a virtual ethernet cable between containers.
    
    ```powershell
    docker network create my-net
    
    ```
    
2. **Run SQL attached to the network:**
Note the `-name` (This becomes the DNS host!).
    
    ```powershell
    docker run -d --name sql-db --network my-net mcr.microsoft.com/mssql/server...
    
    ```
    
3. **Run API attached to the same network:**
    
    ```powershell
    docker run -d --name my-api --network my-net my-optimized-api
    
    ```
    
4. **The Connection String:**
Because they are on the same bridge network, Docker's internal DNS allows you to use the **Container Name** as the **Server Name**.
    - **Old String:** `Server=localhost;...`
    - **New String:** `Server=sql-db;...`

---

### **3. Putting it Together (Manual Integration)**

Let's simulate a full dev environment manually to see the pain points (which Phase 5 solves).

1. **Create Network:**`docker network create app-net`
2. **Start SQL (With Volume & Network):**
    
    ```powershell
    docker run -d --name db-server `
      --network app-net `
      -v db-vol:/var/opt/mssql/data `
      -e "ACCEPT_EULA=Y" -e "SA_PASSWORD=Pass@word1" `
      mcr.microsoft.com/mssql/server:2022-latest
    
    ```
    
3. **Start API (With Network & Connection String Override):**
We override the connection string using Environment Variables (`e`).
    
    ```powershell
    docker run -d --name web-api `
      --network app-net `
      -p 8080:8080 `
      -e "ConnectionStrings__DefaultConnection=Server=db-server;Database=Master;User Id=sa;Password=Pass@word1;TrustServerCertificate=True" `
      my-optimized-api
    
    ```
    

**Result:**
The API talks to `db-server`. The data is stored in `db-vol`.

---

### **Summary of Phase 4**

1. **Ephemeral:** Containers lose data when deleted.
2. **Volumes:** Use `v` to persist DB files to the host disk.
3. **Localhost is a Lie:** `localhost` inside a container does not mean your laptop.
4. **DNS Service Discovery:** Create a custom network. Use container names (`db-server`) as hostnames in your Connection Strings.

**The Pain Point:**
Running these huge Docker commands manually every morning is annoying. Typing out connection strings in PowerShell is prone to errors.

Are you ready to move to **Phase 5: Docker Compose (Local Orchestration)**? (This automates Phase 4 into a single file).

## **Phase 5: Docker Compose (Local Orchestration)**

In Phase 4, we manually typed massive `docker run` commands to connect an API to a Database. That is strictly for learning. **No expert does that in their daily work.**

We use **Docker Compose**.

Docker Compose is a tool that allows you to define your **entire application stack** (API, DB, Redis, etc.) in a single YAML file. It replaces "Imperative Commands" (Do this, then do that) with "Declarative Configuration" (Here is what the system should look like).

---

### **1. The Structure (`docker-compose.yml`)**

This file typically lives in the root of your Solution folder (next to `.sln`).

It has three main sections:

1. **Services:** The containers you want to run (API, SQL).
2. **Volumes:** The persistent storage (Data).
3. **Networks:** (Optional) Compose creates a default network automatically, so we rarely define this manually.

---

### **2. The Expert Compose File**

Here is how a .NET Developer defines an environment with **SQL Server** and a **Web API**, automating everything we did in Phase 4.

**File:** `docker-compose.yml`

```yaml
version: '3.8'

services:
  # ----------------------------------------
  # SERVICE 1: THE DATABASE
  # ----------------------------------------
  db:
    image: mcr.microsoft.com/mssql/server:2022-latest
    container_name: sql-server-container
    environment:
      - ACCEPT_EULA=Y
      - SA_PASSWORD=YourStrong!Passw0rd
    ports:
      - "1433:1433" # Map to host so SSMS works
    volumes:
      - sql_data:/var/opt/mssql/data # Persist data to named volume

  # ----------------------------------------
  # SERVICE 2: THE .NET API
  # ----------------------------------------
  api:
    build:
      context: . # Look for Dockerfile in current folder
      dockerfile: MyApi/Dockerfile
    container_name: my-web-api
    ports:
      - "8080:8080"
    environment:
      # OVERRIDING appsettings.json
      # 1. Use Service Name "db" as Hostname
      # 2. Use __ (Double Underscore) for JSON nesting
      - ConnectionStrings__DefaultConnection=Server=db;Database=Master;User Id=sa;Password=YourStrong!Passw0rd;TrustServerCertificate=True;
      - ASPNETCORE_ENVIRONMENT=Development
    depends_on:
      - db # Wait for 'db' container to exist before starting

volumes:
  sql_data: # Define the volume name here

```

---

### **3. Key .NET Integrations explained**

### **A. Hostnames & Service Discovery**

In the YAML above, we named the database service **`db`**.
Inside the Docker network, the hostname `db` now resolves to the IP address of that container.

- **Result:** `Server=db` works in your C# connection string.

### **B. Environment Variable Injection**

We are using the `environment` section to inject the Connection String.

- **Syntax:** `ConnectionStrings__DefaultConnection`.
- **Behavior:** When [ASP.NET](http://asp.net/) Core starts, it reads Environment variables. It sees the double underscore, translates it to `ConnectionStrings:DefaultConnection`, and overrides whatever is in your `appsettings.json`.

### **C. Persistence**

We declared `volumes: sql_data`.
Even if you run `docker-compose down` (which destroys containers), the `sql_data` volume remains on your disk. Next time you `up`, the DB is restored.

---

### **4. The Commands (The Workflow)**

Stop typing `docker build` and `docker run`. You now use these three commands:

1. **Start Everything:**
    
    ```powershell
    # Builds the images (if needed) and starts containers in background
    docker-compose up -d
    
    ```
    
2. **View Logs:**
    
    ```powershell
    # Tail logs for all services to see startup errors
    docker-compose logs -f
    
    ```
    
3. **Destroy Everything:**
    
    ```powershell
    # Stops and Removes containers and the Network. (Volumes survive)
    docker-compose down
    
    ```
    
4. **Rebuild:**
If you changed C# code, `up` won't automatically recompile because the image already exists. You must force a build:
    
    ```powershell
    docker-compose up -d --build
    
    ```
    

---

### **5. The "Depends On" Trap (Race Conditions)**

**Scenario:** You run `docker-compose up`. The API crashes immediately with `SqlException`. You restart it, and it works.

**The Problem:** `depends_on: - db` only waits for the DB container to **start**. It does NOT wait for SQL Server to be **ready** (which takes 5-10 seconds to initialize logs/memory).
The API starts fast (1 sec), tries to connect, and fails because SQL is still booting.

**The Fixes:**

1. **Resilience (C#):** Implement a Retry Policy (e.g., using **Polly**) in your `.NET` startup code. Retry the DB connection 5 times before crashing. **(Recommended Expert Way)**.
2. **Health Checks (Docker):** Configure a `healthcheck` in the DB service (pinging SQL) and make the API depend on `service_healthy` instead of just `service_started`.

---

### **Summary of Phase 5**

1. **Declarative:** `docker-compose.yml` documents your entire infrastructure.
2. **Service Names = Hostnames:** If you name a service `redis-cache`, your C# code connects to `redis-cache`.
3. **No more manual Flags:** Ports, Envs, and Volumes are written once, committed to Git, and shared with the team.
4. **Dev-Prod Parity:** A new developer joins the team. They clone the repo and run `docker-compose up`. They have the API, DB, and Cache running in 5 minutes. No manual SQL install required.

Are you ready to move to **Phase 6: Optimization (Efficiency)**? (Where we learn to make builds fast and images small using `.dockerignore`).

## **Race Conditions using Health Check**

This is the **Expert** way to solve the "Race Condition" in Docker Compose.

Instead of the API crashing because it tried to connect to SQL Server while SQL was still initializing RAM, we force Docker to **wait until the database actually accepts a query**.

### The Solution: `healthcheck` + `condition: service_healthy`

Here is the updated `docker-compose.yml`. Notice the additions to the `db` service and the change in the `api` service.

```yaml
version: '3.8'

services:
  # ----------------------------------------
  # SERVICE 1: SQL SERVER (The Provider)
  # ----------------------------------------
  db:
    image: mcr.microsoft.com/mssql/server:2022-latest
    container_name: sql-server
    environment:
      - ACCEPT_EULA=Y
      - SA_PASSWORD=YourStrong!Passw0rd
    ports:
      - "1433:1433"

    # --- EXPERT ADDITION: HEALTHCHECK ---
    # Docker runs this command inside the container periodically.
    # If exit code is 0 (Success), status becomes "healthy".
    # If exit code is 1 (Fail), status becomes "unhealthy".
    healthcheck:
      test: ["CMD-SHELL", "/opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P 'YourStrong!Passw0rd' -Q 'SELECT 1' || exit 1"]
      interval: 10s    # Run check every 10 seconds
      timeout: 5s      # If it takes >5s, consider it a fail
      retries: 5       # Fail 5 times before giving up
      start_period: 10s # Give it 10s grace period to boot before checking

  # ----------------------------------------
  # SERVICE 2: .NET API (The Consumer)
  # ----------------------------------------
  api:
    build:
      context: .
      dockerfile: MyApi/Dockerfile
    environment:
      - ConnectionStrings__DefaultConnection=Server=db;Database=Master;User Id=sa;Password=YourStrong!Passw0rd;TrustServerCertificate=True;

    depends_on:
      db:
        # THE FIX: Don't just wait for start. Wait for health!
        condition: service_healthy

```

### Breakdown of what is happening:

1. **The Test Command:**
    - We use `sqlcmd` (a tool already installed inside the Microsoft SQL image).
    - `Q 'SELECT 1'`: This is the standard "Ping". If SQL is capable of executing this, it is ready for your API.
    - `|| exit 1`: If `sqlcmd` fails (e.g., login failed, service down), force an error exit code so Docker knows it failed.
2. **The Wait Strategy:**
    - When you run `docker-compose up`, the `db` container starts.
    - The `api` container sits in a "Created" state (it does **not** start yet).
    - Docker keeps pinging the DB.
    - Once the DB reports "healthy", Docker immediately starts the `api` container.

### Bonus: Healthcheck for your .NET App?

You should also add a health check to your .NET API, so a Load Balancer (or Nginx) knows if your API is alive.

1. **In C# (`Program.cs`):**
    
    ```csharp
    var builder = WebApplication.CreateBuilder(args);
    builder.Services.AddHealthChecks(); // Add Service
    var app = builder.Build();
    app.MapHealthChecks("/health"); // Expose Endpoint
    app.Run();
    
    ```
    
2. **In `docker-compose.yml` (For the API service):**
    
    ```yaml
    api:
      # ... config
      healthcheck:
        # Curl the endpoint defined in C#
        test: ["CMD", "curl", "-f", "<http://localhost:8080/health>"]
        interval: 30s
        timeout: 10s
        retries: 3
    
    ```
    
    *Note: If the standard .NET 8 image doesn't include `curl`, you might need to install it in your Dockerfile, or write a tiny custom C# app to ping itself.*
    

Are you ready to move to **Phase 6: Optimization (Efficiency)**? (Speeding up your Docker Builds).

## **Phase 6: Optimization (Efficiency)**

If your Docker build takes 5 minutes every time you change one line of C# code, you are doing it wrong. Expert Dockerfiles build in **seconds** for minor code changes.

To achieve this, you must understand how Docker "thinks" about lines of code.

---

### **1. Layer Caching (The "Order of Operations")**

Every command in a Dockerfile (`FROM`, `COPY`, `RUN`) creates a **Layer**.
Docker caches these layers. When you rebuild, Docker checks: "Has the input for this line changed?"

- **No?** Use the cached layer (Instant).
- **Yes?** Re-run this line **and every single line after it.**

### **The "Slow" (Novice) Way**

Look at this sequence. This is the most common mistake.

```docker
WORKDIR /app
COPY . .               # <--- 1. Copies EVERYTHING (.csproj, .cs files, everything)
RUN dotnet restore     # <--- 2. Downloads internet packages
RUN dotnet publish     # <--- 3. Compiles code

```

**Scenario:** You change **one** comment in `Program.cs`.

1. Docker sees `COPY . .` has changed (because `Program.cs` is part of `.`).
2. Docker **invalidates** Layer 1.
3. Docker **must re-run** Layer 2 (`dotnet restore`).
4. **Result:** You wait 2 minutes for NuGet restore, even though you didn't add any new packages!

### **The "Fast" (Expert) Way**

We split the copy command based on how often files change. `csproj` files change rarely. `.cs` source files change constantly.

```docker
WORKDIR /src

# 1. Copy ONLY the project file(s) first
COPY ["MyApi/MyApi.csproj", "MyApi/"]

# 2. Restore Dependencies
# Docker will CACHE this layer indefinitely until you actually touch the csproj
RUN dotnet restore "MyApi/MyApi.csproj"

# 3. NOW copy the source code
COPY . .

# 4. Build
RUN dotnet publish ...

```

**Scenario:** You change **one** comment in `Program.cs`.

1. Docker checks Layer 1 (`COPY csproj`). **Unchanged.** Uses Cache.
2. Docker checks Layer 2 (`restore`). Input (csproj) is Unchanged. **Uses Cache (Instant!).**
3. Docker checks Layer 3 (`COPY . .`). **Changed.** Runs this.
4. Docker runs Layer 4 (`publish`).
5. **Result:** Build completes in seconds. You skipped the network heavy restore step entirely.

---

### **2. The Context Diet (`.dockerignore`)**

When you run `docker build .`, the first thing Docker says is:
`=> [internal] load build definition from Dockerfile=> [internal] load .dockerignore=> [internal] transferring context: 512.45MB`  <-- **WARNING**

**The Problem:**
Docker is taking **every single file** in your folder and sending it to the Docker Daemon (Context) before it even looks at the `COPY` command.
If you have local `bin`, `obj`, or `.git` folders on your laptop, you are sending hundreds of megabytes of garbage to the Docker engine unnecessarily.

**The Solution:**
Add a `.dockerignore` file in the root of your project. It works exactly like `.gitignore`.

**Standard .NET `.dockerignore`:**

```
**/.dockerignore
**/.git
**/.vs
**/.vscode
**/bin
**/obj
**/TestResults

```

**Benefits:**

1. **Speed:** "Transferring context" drops from 500MB to <1MB.
2. **Safety:** Prevents you from accidentally overwriting `dotnet restore` behavior by copying your local (potentially corrupted or wrong-OS) `bin` folders into the container.
3. **Secrets:** Prevents copying local `.env` files with keys into the image.

---

### **3. Base Image Size (Alpine vs. Debian/Chiseled)**

Size matters when pushing to Azure Container Registry (ACR) and pulling to Kubernetes nodes.

- **Standard (`mcr.microsoft.com/dotnet/aspnet:8.0`):**
    - OS: Debian.
    - Size: ~200MB (uncompressed).
    - Best for: Compatibility. Includes standard Linux tools (`curl`, `apt`).
- **Alpine (`8.0-alpine`):**
    - OS: Alpine Linux (BusyBox).
    - Size: ~100MB (uncompressed).
    - Best for: Maximum shrinking.
    - *Expert Note:* Alpine uses `musl` libc instead of `glibc`. Rarely, some .NET native dependencies (like generic image processing libs or older SQL clients) crash on Alpine. Test thoroughly.
- **Chiseled (`8.0-jammy-chiseled`):**
    - OS: Ubuntu (Stripped).
    - Size: ~110MB.
    - **The Gold Standard:** It removes the shell (`bash`), package manager (`apt`), and running as root. It is ultra-secure because an attacker cannot "SSH in" or run commands inside it.

---

### **Summary of Phase 6**

1. **Cache the Restore:** Always `COPY *.csproj` -> `dotnet restore` -> `COPY .` -> `dotnet publish`.
2. **Ignore Garbage:** Use `.dockerignore` to block `bin`/`obj`/`.git`.
3. **Choose Your Base:** Start with standard. Move to Chiseled or Alpine if you need smaller storage/faster scaling.

Are you ready to move to **Phase 7: Security & Production (Expert)**? (Where we prepare the image for the harsh reality of the Cloud).

## **Phase 7: Security & Production (Expert)**

You know how to build small, fast images. Now we need to make them **Secure** and ready for **Azure**.

Running a container in production is very different from `localhost`. Security teams will audit your images, and using defaults (like running as Root) will get your deployment rejected.

---

### **1. Non-Root Execution (`USER`)**

**The Vulnerability:**
By default, Docker containers run as `root`.
If a hacker finds a vulnerability in your [ASP.NET](http://asp.net/) Core app (e.g., Remote Code Execution), they effectively have `root` privileges inside the container. From there, it is much easier to "break out" of the container and attack the host node (Azure Kubernetes Node).

**The Solution:**
Switch to a low-privilege user immediately after setting up the app.

**Note on .NET 8+ Changes:**
Starting with .NET 8, the Microsoft base images allow running as a non-root user more easily by changing the default port from `80` (privileged) to `8080` (non-privileged).

**The Expert Dockerfile Implementation:**

```docker
FROM mcr.microsoft.com/dotnet/aspnet:8.0 AS final
WORKDIR /app

# 1. Switch User
# 'app' is a built-in user created by Microsoft in .NET 8 images (uid=1654).
# Once you run this line, you CANNOT run 'apt-get' or write to system folders.
USER app

# 2. Expose the Non-Privileged Port
# .NET 8 listens on 8080 by default now.
EXPOSE 8080

COPY --from=build /app/publish .
ENTRYPOINT ["dotnet", "MyApi.dll"]

```

---

### **2. Hardening: Chiseled Images**

We touched on this in Optimization, but it is primarily a **Security** feature.

**The Philosophy:** "You cannot use a tool that doesn't exist."
If a hacker exploits your app and tries to run `bash` or `curl` to download malware, they will fail if your container **doesn't have bash or curl**.

**Implementation:**
Change your final stage to use the Ubuntu "Chiseled" base.

```docker
# Regular:
# FROM mcr.microsoft.com/dotnet/aspnet:8.0 AS final

# Secure (Chiseled):
FROM mcr.microsoft.com/dotnet/aspnet:8.0-jammy-chiseled AS final

```

*Note:* If you use Chiseled images, you **cannot** use `shell` commands in your `healthcheck` (because `sh` doesn't exist). You must use a built-in health check binary or language-level probing.

---

### **3. The "Latest" Tag Trap**

**Novice Behavior:**
Deploying `myapp:latest` to production.

**The Disaster:**

1. You push `myapp:latest` on Tuesday (v1). Azure pulls it. It works.
2. You push `myapp:latest` on Wednesday (v2) which has a bug.
3. Azure scales up (adds a node) and pulls `myapp:latest`. The new node gets v2 (Broken), while the old node runs v1 (Working).
4. **You now have a "Split Brain" cluster and cannot easily rollback.**

**Expert Behavior:Immutable Tags.** Every production build gets a unique ID.

- `myregistry.azurecr.io/myapi:build-1234`
- `myregistry.azurecr.io/myapi:v1.0.2`

If v1.0.2 is broken, you simply update your YAML config to point back to `v1.0.1`.

---

### **4. Azure Container Registry (ACR) Workflow**

To get your image from Laptop to Azure, you don't send a zip. You "Push" the image.

**1. Login:**

```powershell
# Authenticates your local Docker CLI with Azure
az acr login --name MyCompanyRegistry

```

**2. Tagging:**
You must "rename" your local image to include the full registry URL.

```powershell
# Syntax: docker tag [LocalName] [RegistryUrl]/[Name]:[Tag]
docker tag my-api mycompany.azurecr.io/sales-api:v1.0

```

**3. Pushing:**

```powershell
docker push mycompany.azurecr.io/sales-api:v1.0

```

---

### **5. Database Migrations (The "Init" Problem)**

**The Scenario:**
You use EF Core (`context.Database.Migrate()`) inside `Program.cs`.

**The Production Problem:**
If you start 5 Replicas (Copies) of your API in Kubernetes simultaneously, **all 5 containers** wake up and try to apply migrations to the SQL DB at the exact same millisecond.
**Result:** DB Locks, Transaction Failures, Data Corruption.

**The Expert Solution: Init Containers / Migration Bundles**
Do NOT migrate in the app startup in Production.

1. **Generate a Bundle (during CI):**`dotnet ef migrations bundle --self-contained -r linux-x64`
This produces a single executable file named `efbundle`.
2. **Package it:** Put this executable in a tiny Docker image.
3. **Run it once:** Run this container *before* your main API containers start (using Kubernetes InitContainers or a Pipeline Task).

---

### **Final Expert Challenge: The "Perfect" Dockerfile**

This file combines **Phases 3, 6, and 7**. If you can write and explain this, you are a Docker Expert for .NET.

```docker
# -------------------------------------------------------------------------
# STAGE 1: Build & Publish (Uses SDK)
# -------------------------------------------------------------------------
FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build
WORKDIR /src

# OPTIMIZATION: Copy csproj first for Layer Caching
COPY ["src/MyApi/MyApi.csproj", "src/MyApi/"]
COPY ["src/MySharedLib/MySharedLib.csproj", "src/MySharedLib/"]

# RESTORE: Run restore against the csproj (Cached)
RUN dotnet restore "src/MyApi/MyApi.csproj"

# COPY: Copy the rest of the code
COPY . .

# BUILD: Publish to the /app folder
WORKDIR "/src/src/MyApi"
RUN dotnet publish "MyApi.csproj" -c Release -o /app/publish /p:UseAppHost=false

# -------------------------------------------------------------------------
# STAGE 2: Production Runtime (Uses Chiseled + Non-Root)
# -------------------------------------------------------------------------
# SECURITY: Use Chiseled image (No shell, tiny footprint)
FROM mcr.microsoft.com/dotnet/aspnet:8.0-jammy-chiseled AS final
WORKDIR /app

# NETWORKING: 8080 is standard for .NET 8 non-root
EXPOSE 8080

# COPY: Get artifacts from Stage 1
COPY --from=build /app/publish .

# EXECUTION: Defined entrypoint
ENTRYPOINT ["dotnet", "MyApi.dll"]

```

### **Congratulations!**

You have moved from "Running `docker run` blindly" to architecting secure, optimized, multi-stage images ready for Azure Enterprise environments.

**What to do next?**
Take this knowledge and look at **Azure Kubernetes Service (AKS)** or **Azure Container Apps (ACA)**, which are the hosting platforms designed to run these specific images.

# Configurations

---

Of course. Mastering how to get configuration into your containers is one of the most fundamental Docker skills. Let's do a deep dive into the key commands and keywords you will use every single day, with a focus on a .NET developer's perspective.

---

### **1. `ENV` (in the Dockerfile)**

The `ENV` instruction sets an **environment variable** that exists **inside the image itself**. It gets baked into the image layers.

### **What is its job?**

To set a default value that will be present every time *any* container is started from this image, unless it is overridden. It's for defining non-secret, application-wide defaults.

### **Syntax:**

```docker
# Syntax: ENV <key>=<value> (for a single variable)
# Or:     ENV <key1>=<value1> <key2>=<value2> ...

# Example: Set the default ASP.NET Core environment and listening ports
ENV ASPNETCORE_ENVIRONMENT=Production
ENV ASPNETCORE_URLS=http://+:8080

```

### **How it works:**

- When a container starts from this image, the process inside (your `.NET` app) will see these two environment variables as if they were set by the operating system.
- [ASP.NET](http://asp.net/) Core automatically reads `ASPNETCORE_ENVIRONMENT` and configures itself for "Production" mode (e.g., disables detailed error pages).
- It also reads `ASPNETCORE_URLS` and tells the Kestrel web server to listen on port 8080.

### **When to use `ENV`:**

- For **non-secret defaults** that are intrinsic to how the application should run.
- To set language or path variables (e.g., `ENV LANG=C.UTF-8`).
- **Crucially, this is NOT for secrets!** The values are visible to anyone who inspects the image (`docker inspect my-image`).

---

### **2. `ARG` (in the Dockerfile)**

The `ARG` instruction defines a **build-time variable**. It only exists while the `docker build` command is running. It does **not** exist inside the final image or the running container.

### **What is its job?**

To parameterize your `Dockerfile`, allowing you to pass in values from the command line during the build process.

### **Syntax:**

```docker
# Syntax: ARG <name>
# Or:     ARG <name>=<default_value>

# Example: Allow specifying a base image version at build time
ARG DOTNET_VERSION=8.0
FROM mcr.microsoft.com/dotnet/sdk:${DOTNET_VERSION} AS build

```

### **How to use it:**

You pass a value to the argument using the `--build-arg` flag in your `docker build` command.

```bash
# This will use the default value of 8.0
docker build -t my-app .

# This will override the default and use the 7.0 SDK image instead
docker build --build-arg DOTNET_VERSION=7.0 -t my-app .

```

### **`ENV` vs. `ARG` (The "Expert" Trap):**

An `ARG` is useless at runtime. An `ENV` is visible at runtime. What if you want to pass a build argument and have it persist as an environment variable?

**The Pattern:**

```docker
# 1. Define the build-time argument
ARG APP_VERSION_ARG=1.0.0

# 2. Use the argument to set a permanent environment variable
ENV APP_VERSION_ENV=${APP_VERSION_ARG}

```

Now you can run `docker build --build-arg APP_VERSION_ARG=1.2.3` and the resulting container will have an environment variable `APP_VERSION_ENV` with the value `1.2.3`.

---

### **3. The `e` / `-env` Flag (at `docker run`)**

This is the standard way to provide **runtime configuration** to a container when you start it. This is how you pass in **secrets and environment-specific settings**.

### **What is its job?**

To set an environment variable *inside a specific container*. This value **overrides** any default value that was set with an `ENV` instruction in the Dockerfile.

### **Syntax:**

```bash
# Syntax: -e KEY=VALUE
# Or:     --env KEY=VALUE
# Or:     -e KEY (this will take the value from your local shell's environment)

```

### **How it Works (The .NET Example):**

Your `appsettings.json` has a connection string for local development. Your Dockerfile sets `ENV ASPNETCORE_ENVIRONMENT=Production`. You need to provide the production connection string at runtime.

```bash
docker run -d -p 8080:8080 \\
  -e "ASPNETCORE_ENVIRONMENT=Staging" \\
  -e "ConnectionStrings__DefaultConnection=Server=prod-db;User=...;Password=..." \\
  my-api-image

```

- **The Magic:** [ASP.NET](http://asp.net/) Core's configuration system automatically reads these environment variables. Because they are loaded *after* `appsettings.json`, they take precedence. The `ConnectionStrings__DefaultConnection` variable (with the double underscore) correctly overrides the nested JSON key `ConnectionStrings:DefaultConnection`.

### **The `-env-file` Flag**

If you have many environment variables, typing them all with `-e` is tedious. You can put them in a file.

**`prod.env` file:**

```
ASPNETCORE_ENVIRONMENT=Production
ConnectionStrings__DefaultConnection=Server=prod-db;...
Logging__LogLevel__Default=Warning

```

**`docker run` command:**

```bash
docker run -d -p 8080:8080 --env-file ./prod.env my-api-image

```

---

### **Summary Table for Day-to-Day Work**

| Keyword | File/Command | Exists During... | Primary Use Case for a .NET Developer |
| --- | --- | --- | --- |
| **`ENV`** | **Dockerfile** | **Runtime** | Setting **non-secret, default** values for your application, like `ASPNETCORE_ENVIRONMENT=Production`. |
| **`ARG`** | **Dockerfile** | **Build-time only** | Parameterizing your `Dockerfile`. For example, letting your CI pipeline specify which base image version to use (`--build-arg`). |
| **`-e` / `--env`** | **`docker run`** or **`docker-compose.yml`** | **Runtime** | **The standard way to provide secrets and environment-specific configuration**. Overriding connection strings, API keys, etc. |

### **The "Golden Rule" Workflow**

For a professional .NET project, your workflow should be:

1. **`Dockerfile` `ENV`:** Use this for **static, non-secret** settings that define the container's environment (e.g., `ASPNETCORE_URLS`).
2. **`appsettings.json`:** Use this for **local development defaults** and general configuration structure.
3. **`docker-compose.yml` / `docker run -e` / Kubernetes Secrets:** Use these to **inject secrets and override settings** for a specific environment (Dev, QA, Prod). Your production connection string should **never** be in your Dockerfile. It should always be injected at runtime.

# ENV File in Docker Compose

---

Excellent question. This is a core part of creating a flexible and realistic local development setup with Docker Compose. You want to be able to switch between "development mode" and a "production-like mode" without changing your `docker-compose.yml` file.

The key to this is using `.env` files and understanding Docker Compose's order of precedence for configuration.

---

### **1. How Docker Compose Uses `.env` Files**

By default, when you run `docker-compose up`, it automatically looks for a file named **`.env`** in the **same directory** where you are running the command.

- **What it does:** Docker Compose reads the key-value pairs from this `.env` file and substitutes them into your `docker-compose.yml` file wherever you use the `${VARIABLE_NAME}` syntax.
- **Crucially:** These variables are used for **Compose file parsing only**. They are *not* automatically passed into your containers as environment variables unless you explicitly tell Compose to do so.

---

### **2. The Pattern: A Default `.env` and Override Files**

The standard pattern is to have a default `.env` file for your local development setup and then use override files for different environments like production or CI.

### **Step 1: The `docker-compose.yml` File**

Let's create a compose file that defines our API. Notice the `${...}` placeholders. These will be filled in by our `.env` files.

```yaml
# docker-compose.yml
version: '3.8'

services:
  api:
    build:
      context: .
      dockerfile: MyApi/Dockerfile
    ports:
      # We will get the port mapping from an env variable.
      - "${API_PORT_HOST}:${API_PORT_CONTAINER}"
    environment:
      # We will pass these variables directly into the container.
      - ASPNETCORE_ENVIRONMENT=${ASPNETCORE_ENVIRONMENT}
      - ConnectionStrings__DefaultConnection=${DB_CONNECTION_STRING}

```

### **Step 2: The Default `.env` File (for Development)**

This file sits right next to `docker-compose.yml`. It's your "go-to" for local debugging. **You can commit this file to Git** as it contains non-secret development values.

**File:** `.env`

```
# --- Development Environment Configuration ---

# Port mapping for local dev
API_PORT_HOST=8000
API_PORT_CONTAINER=8080

# Environment variables for inside the container
ASPNETCORE_ENVIRONMENT=Development
DB_CONNECTION_STRING=Server=localhost;Database=DevDb;...

```

**How it works:**

- When you run `docker-compose up`, Compose reads this file.
- It sees `API_PORT_HOST=8000` and replaces `${API_PORT_HOST}` with `8000`. The final port mapping becomes `8000:8080`.
- It sees `ASPNETCORE_ENVIRONMENT=Development` and injects this into the container, so your .NET app starts in Development mode.

---

### **3. Dynamic Selection (The `prod.env` File)**

Now, let's create a file for a production-like scenario. This file contains the configuration you would use in a real deployment. **You should NOT commit this file to Git** if it contains real secrets.

**File:** `prod.env`

```
# --- Production Environment Configuration ---

# Port mapping for production-like test
API_PORT_HOST=80
API_PORT_CONTAINER=8080

# Environment variables for inside the container
ASPNETCORE_ENVIRONMENT=Production
DB_CONNECTION_STRING=Server=prod-sql-server.database.windows.net;...

```

### **How to Select the `prod.env` File**

You use the `--env-file` command-line flag when running `docker-compose`. This tells Compose to use a *different* file instead of the default `.env`.

**The Command:**

```bash
# Start the containers using the settings from 'prod.env'
docker-compose --env-file prod.env up -d

```

- Now, Compose ignores `.env` and reads `prod.env`.
- The port mapping will be `80:8080`.
- The `ASPNETCORE_ENVIRONMENT` inside the container will be set to `Production`.

This allows you to test different configurations using the exact same `docker-compose.yml` and Docker image.

---

### **4. A More Advanced Pattern: Overriding with `e`**

Command-line variables have an even higher precedence. You can use this for dynamic values, like a build number from your CI system.

**Docker Compose Precedence Order (from lowest to highest):**

1. Default `.env` file.
2. `-env-file` specified file.
3. `e` command-line arguments.
4. Shell environment variables from the machine running `docker-compose`.

**Example:**
Imagine your CI system (like Azure Pipelines) needs to inject a unique build number.

**CI Pipeline Script:**

```bash
# The 'BUILD_NUMBER' is a variable provided by the CI system (e.g., '2023.11.15.1')
export APP_VERSION=$BUILD_NUMBER

# Run compose. It will use prod.env, but the APP_VERSION from the shell
# will override any value in the file.
docker-compose --env-file prod.env up -d

```

You would need to have an `APP_VERSION` variable in your `docker-compose.yml` `environment` section for this to work.

### **Final Summary: The Workflow**

1. **Define placeholders** in `docker-compose.yml` using `${VARIABLE_NAME}` for things that change between environments (ports, connection strings, etc.).
2. Create a **default `.env` file** for your standard local development setup. Commit this.
3. Create **separate `.env` files** (like `ci.env` or `prod.env`) for other configurations. Add these to `.gitignore`.
4. Use `docker-compose up` for your normal day-to-day work.
5. Use `docker-compose --env-file <your_file>.env up` to test a specific configuration.

This pattern provides a powerful, flexible, and repeatable way to manage your application stack across different environments.